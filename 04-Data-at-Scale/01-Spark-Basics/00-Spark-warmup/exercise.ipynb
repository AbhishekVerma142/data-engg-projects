{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ The goal of the exercise is to learn pyspark syntax and will involve 5 questions tested directly in the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will create the spark session and load the data in for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Olist Exercises\").getOrCreate()\n",
    "\n",
    "df_customers = spark.read.csv(\"data/olist_customers_dataset.csv\", header=True, inferSchema=True)\n",
    "df_geolocation = spark.read.csv(\"data/olist_geolocation_dataset.csv\", header=True, inferSchema=True)\n",
    "df_items = spark.read.csv(\"data/olist_order_items_dataset.csv\", header=True, inferSchema=True)\n",
    "df_payments = spark.read.csv(\"data/olist_order_payments_dataset.csv\", header=True, inferSchema=True)\n",
    "df_reviews = spark.read.csv(\"data/olist_order_reviews_dataset.csv\", header=True, inferSchema=True)\n",
    "df_orders = spark.read.csv(\"data/olist_orders_dataset.csv\", header=True, inferSchema=True)\n",
    "df_products = spark.read.csv(\"data/olist_products_dataset.csv\", header=True, inferSchema=True)\n",
    "df_sellers = spark.read.csv(\"data/olist_sellers_dataset.csv\", header=True, inferSchema=True)\n",
    "df_category_translation = spark.read.csv(\"data/product_category_name_translation.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Caculate the count of each of the order statuses and save the result as `df_order_status_count`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ **Testing** Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "pandas_df_order_status_count = df_order_status_count.toPandas()\n",
    "\n",
    "result = ChallengeResult('order_status_count',\n",
    "    order_status_count_df=pandas_df_order_status_count\n",
    ")\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Calculate the Customer Lifetime Value for each customer. CLV is calculated as the sum of the payment values for each customer. Save the resulting spark DataFrame into a variable called `df_clv`. The clv column should be named `CLV`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ **Testing** Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "df_clv_pd = df_clv.toPandas()\n",
    "\n",
    "result = ChallengeResult('customer_clv',\n",
    "    df_clv_pd=df_clv_pd\n",
    ")\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Calculate the total revenue for each seller. Save the resulting spark DataFrame in a variable called `df_seller_revenue`. The total revenue column should be named `Total_Revenue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "df_seller_revenue_pd = df_seller_revenue.toPandas()\n",
    "\n",
    "result = ChallengeResult('seller_revenue',\n",
    "    df_seller_revenue_pd=df_seller_revenue_pd\n",
    ")\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Identify the top 10 product categories with the highest **count** of canceled or unavailable orders. Save the result in a spark DataFrame called `df_high_cancel_categories`. The column for total instances should be named `Total_Canceled_Unavailable`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "df_high_cancel_categories_pd = df_high_cancel_categories.toPandas()\n",
    "\n",
    "result = ChallengeResult('high_cancel_categories',\n",
    "    df_high_cancel_categories_pd=df_high_cancel_categories_pd\n",
    ")\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Calculate the average extended delivery time for each product category. The extended delivery time is the actual delivery time minus the estimated delivery time. Save the result in a variable called `df_delivery_time`. The `Avg_Extended_Delivery` column should contain the number of hours (as a float), sorted in descending order, and limited to the top 10 product categories with the highest average extended delivery time.\n",
    "\n",
    "<details>\n",
    "    <summary>üí°Hint</summary>\n",
    "\n",
    "Break up the transformation into a few steps! Try creating an `actual_delivery_time` and `estimated_delivery_time` on a DataFrame that makes sense.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "df_delivery_time_pd = df_delivery_time.toPandas()\n",
    "\n",
    "result = ChallengeResult('delivery_time',\n",
    "    df_delivery_time_pd=df_delivery_time_pd\n",
    ")\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished üèÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have finished the notebook you should have a good grounding of how to use pyspark!\n",
    "\n",
    "You can run \n",
    "\n",
    "```bash\n",
    "make test\n",
    "```\n",
    "\n",
    "Make sure to commit and push all your changes to github so you can track your progress on Kitt!\n",
    "\n",
    "Time to move on to the next exercise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
